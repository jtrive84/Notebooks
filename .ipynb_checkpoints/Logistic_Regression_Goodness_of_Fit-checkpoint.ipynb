{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goodness of Fit and Significance Testing for Logistic Regression Models    \n",
    "<br>\n",
    "Author: James D. Triveri   \n",
    "<br>\n",
    "\n",
    "\n",
    "\n",
    "In an earlier [post]({filename}/articles/Statistical_Modeling/Logistic_Estimation.md), we implemented the Fisher Scoring algorithm, which we then used to estimate the coefficients for a Logistic Regression model. We demonstrated that our algorithm's coefficient estimates were identical to the coefficients produced by R's `glm` function, but we didn't formally assess the quality of the model in any way. In this post, we'll determine the goodness of fit of our Logistic Regression model and test the the significance of our coefficient estimates. \n",
    "\n",
    "\n",
    "## Deviance\n",
    "\n",
    "[Deviance](https://en.wikipedia.org/wiki/Deviance_(statistics)) is used as goodness of fit measure for Generalized Linear Models, and in cases when parameters are estimated using maximum likelihood, is a generalization of the residual sum of squares in Ordinary Least Squares Regression. The deviance of a fitted model compares the log-likelihood of the model of interest to the log-likelihood of the *saturated model*, the model with n parameters that fits the n observations perfectly, but is overparameterized to the point that it is basically just interpolating the data.\n",
    "\n",
    "In large samples, deviance follows a chi-square distribution with $n-p$ degrees of freedom, where $n$ is the number of observations and $p$ is the number of parameters in the model. The null hypothesis, $H_{0}$, is that the model fits. The alternative hypothesis, $H_{1}$, is that the model does not fit. A deviance much higher than $n-p$ indicates the model is a poor fit to the data. Quantifiably, smaller is always better: The smaller the deviance, the better the fit of the model.\n",
    "\n",
    "\n",
    "Let:\n",
    "\n",
    "*  $\\hat{L}$ = the likelihood for the current model of interest      \n",
    "*  $\\hat{L}_{max}$ = the likelihood of the saturated model, the model having n parameters which fit the n observations perfectly        \n",
    "*  $\\hat{l}$ = the log-likelihood for the current model of interest       \n",
    "*  $\\hat{l}_{max}$ = the log-likelihood of the saturated model, the model having n parameters which fit the n observations perfectly        \n",
    "\n",
    "\n",
    "Then the *deviance* can be represented as:\n",
    "\n",
    "$$\n",
    "D = -2Ln \\Big(\\frac{\\hat{L}}{\\hat{L}_{max}}\\Big) = -2(\\hat{l} - \\hat{l}_{max})\n",
    "$$\n",
    "\n",
    "As demonstrated [here](({filename}/articles/Statistical_Modeling/Logistic_Estimation.md), the binomial log-likelihood function is:\n",
    "\n",
    "$$\n",
    "l(\\hat{\\beta}) = \\sum_{i=1}^n \\big(y_{i}Ln(\\pi_{i}) + (1-y_{i})Ln(1 - \\pi_{i})\\big)\n",
    "$$\n",
    "\n",
    "For the saturated model, $\\pi_{i}=y_{i}$, and for the model of interest, $\\pi_{i} = \\hat{y}_{i}$. Thus, the the deviance is represented as:\n",
    "\n",
    "$$\n",
    "D = 2\\sum_{i=1}^n \\Big(y_{i} Ln\\frac{y_{i}}{\\hat{y}_{i}}+(1-y_{i})Ln \\frac{1 - y_{i}}{1 - \\hat{y}_{i}}\\Big)\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "## Calculation\n",
    "\n",
    "For the example in this post, we'll be using [PlacekickerData.csv](https://gist.github.com/jtrive84/f4185fcbf02a92138b76d8ae56e5ffad), a data set with a dichotomous response indicating the success or failure of all placekicks in the 1995 NFL season [1](#Footnotes:). What follows is a brief description of the fields in *PlacekickerData.csv*:\n",
    "    \n",
    "*  **WEEK**: Week of the NFL season     \n",
    "*  **DISTANCE**: Distance in yards of the placekick    \n",
    "*  **CHANGE**: Whether success of the placekick would have resulted in a lead-change (1) or not (0)   \n",
    "*  **ELAP30**: Minutes remaining before trhe end of the half    \n",
    "*  **PAT**: Point After Attempt (1) or field goal (0)   \n",
    "*  **TYPE**: Outdoor (1) or indoor (0)   \n",
    "*  **FIELD**: Grass (1) or artificial turf (0)           \n",
    "*  **WIND**:Whether placekick was attepted in windy conditions (1) or not (0)\n",
    "*  **GOOD**: The response variable, (1) if placekick is successful, otherwise (0)           \n",
    "\n",
    "We will model *GOOD* as a function of *DISTANCE* and *WIND*.First read the dataset into R then pass the model specification to `glm` in order to generate the model summary statistics:\n",
    "              \n",
    "```R\n",
    "df <- read.table(\n",
    "             file=\"PlacekickerData.csv\",\n",
    "             header=TRUE, \n",
    "             sep=\",\", \n",
    "             stringsAsFactors=FALSE\n",
    "             )\n",
    "\n",
    "\n",
    "pkfit <- glm(\n",
    "          formula=GOOD ~ DISTANCE + WIND,\n",
    "          family=binomial(link=logit),\n",
    "          data=df\n",
    "          )\n",
    "```\n",
    "\n",
    "Calling `summary(pkfit)` returns:\n",
    "   \n",
    "```R\n",
    "> summary(pkfit)\n",
    "\n",
    "glm(formula = GOOD ~ DISTANCE + WIND, family = binomial(link = logit), \n",
    "    data = df2)\n",
    "\n",
    "Deviance Residuals: \n",
    "    Min       1Q   Median       3Q      Max  \n",
    "-2.7662   0.2353   0.2353   0.3706   1.5914  \n",
    "\n",
    "Coefficients:\n",
    "             Estimate Std. Error z value Pr(>|z|)    \n",
    "(Intercept)  5.884528   0.331916  17.729   <2e-16 ***\n",
    "DISTANCE    -0.115588   0.008396 -13.767   <2e-16 ***\n",
    "WIND        -0.582124   0.314051  -1.854   0.0638 .  \n",
    "---\n",
    "Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1\n",
    "\n",
    "(Dispersion parameter for binomial family taken to be 1)\n",
    "\n",
    "    Null deviance: 1013.43  on 1424  degrees of freedom\n",
    "Residual deviance:  772.53  on 1422  degrees of freedom\n",
    "AIC: 778.53\n",
    "```\n",
    "    \n",
    "The following sections highlight the calculations used in generating the model summary statistics.\n",
    "      \n",
    "### Residual Deviance\n",
    "We'll begin by reproducing the *Residual Deviance* (or simply the *deviance*) output, which, for the model in question, has a value of $772.53$. To do so, we make use of the fact that specific to Logistic Regression models, the likelihood of the saturated model is 1, which means the saturated model's log-likelihood is 0. Thus the original Deviance expression, $D = 2(l_{max} - l)$ reduces to $D = -2l$. The deviance is computed by multiplying the model of interest's log-likelihood by $-2$:\n",
    "      \n",
    "```R\n",
    "# recall that pkfit resulted from calling glm on `PlacekickerData.csv`\n",
    "# with model specification GOOD ~ DISTANCE + WIND =>\n",
    "fitted <- unname(pkfit$fitted.values)\n",
    "actual <- df$GOOD\n",
    "\n",
    "\n",
    "# function to compute the log-likelihood\n",
    "# y  => actual values (1/0)\n",
    "# pi => fitted probabilities\n",
    "ll <- function(y, pi) return(y*log(pi)+(1-y)*log(1-pi))\n",
    "\n",
    "D <- -2 * sum(ll(y=actual, pi=fitted))\n",
    "\n",
    "print(D)\n",
    "# yields 772.5335\n",
    "\n",
    "```\n",
    "     \n",
    "Looking back at *Residual deviance* in the original model summary, we note that the results are identical ($772.53$ from the summary vrs. $772.5335$ here). \n",
    "\n",
    "To test the goodness of fit of the model, recall that the null hypothesis is that the model is correctly specified. Pass the residual deviance, $772.5335$ along with the model degrees of freedom to `pchisq` to determine whether there is strong evidence to reject the null hypothesis:\n",
    "\n",
    "```R\n",
    "df       <- 1422\n",
    "deviance <- 772.5335\n",
    "p_val    <- pchisq(deviance, df=df, lower.tail=FALSE)\n",
    "\n",
    "print(p_val)\n",
    "# returns 1\n",
    "```\n",
    "\n",
    "There is no evidence to reject the null hypothesis that the model fits. \n",
    "Also note that the model passes the 'off-the-cuff' goodness of fit test since the deviance is less than $n-p$, $772.5335 < 1425-3 = 1422$.\n",
    "\n",
    "\n",
    "          \n",
    "### Null Deviance\n",
    "The *null deviance* shows how well the model with nothing but an intercept predicts the response. If the null deviance is really small, it means that the intercept-only model sufficiently explains the data.    \n",
    "To calculate *null deviance*, the same approach used to calculate residual deviance can be leveraged by simply replacing the fitted values with the mean over all values of the response:\n",
    "\n",
    "```R\n",
    "# recall that pkfit resulted from calling glm on `PlacekickerData.csv`\n",
    "# with model specification GOOD ~ DISTANCE + WIND =>\n",
    "actual <- df$GOOD\n",
    "ravg   <- mean(df$GOOD) # 0.885614\n",
    "\n",
    "# function to compute the log-likelihood\n",
    "# y  => actual values (1/0)\n",
    "# pi => fitted probabilities\n",
    "ll <- function(y, pi) return(y*log(pi)+(1-y)*log(1-pi))\n",
    "\n",
    "nullDeviance <- -2 * sum(ll(y=actual, pi=ravg))\n",
    "\n",
    "print(nullDeviance)\n",
    "# yields 1013.426\n",
    "\n",
    "```\n",
    "The output matches the *Null deviance* from the model summary, which was $1013.43$.\n",
    "           \n",
    "### Comparison of Null and Residual Deviance \n",
    "\n",
    "The difference between log-likelihoods for two models (the model with more parameters identified as the *full model*, the other the *reduced model*), one being a proper subset of the other, has an approximate chi-square distribution in large samples with degrees of freedom equal to the difference in the number of parameters between the full and reduced models. It can also be shown that the difference in deviances between the full and reduced model also follows a chi-square distribution with degrees of freedom equal to the difference in the number of parameters between the full and reduced models:\n",
    "\n",
    "$$\n",
    "-2Ln(L_{reduced})--2Ln(L_{full}) = -2Ln\\Big(\\frac{L_{reduced}}{L_{full}}\\Big) = D_{reduced} - D_{full} \\sim \\chi^{2}(q),\n",
    "$$\n",
    "\n",
    "where $q$ represents the degrees of freedom. The statistic on the left in the expression is the *likelihood ratio statistic*. By comparing the residual and null deviances, we can assess the significance of the parameters in the full model that are not present in the null model. Large values of the chi-square statistic are taken as evidence that the the null hypothesis is implausible.\n",
    "     \n",
    "For the reduced model (same as the null model in this example), the degrees of freedom are the number of observations minus one for the intercept term, $df_{reduced} = n - 1$. For the full model, the degrees of freedom are the number of observations minus the number of parameters including the intercept, or $df_{full} = n - 3$. Thus, for significance test, $df = df_{full} - df_{reduced} = n - 1 - n + 3 = 2$.\n",
    "\n",
    "The null hypothesis, $H_{0}$, states that $\\beta_{DISTANCE} = \\beta_{WIND} = 0$. The alternative hypothesis, $H_{1}$, states that either $\\beta_{DISTANCE}$ or $\\beta_{WIND}$ (or both) are non-zero.      \n",
    "\n",
    "(Note that you can test each explanatory variable (`DISTANCE` and `WIND` here) separately against the intercept-only model in order to isolate their respective contribution to the residual deviance, but in this example, we're testing their combined significance)\n",
    "\n",
    "If the difference between null and residual deviances is close to zero, it's an indication that `DISTANCE` and `WIND` are not significant. Alternatively, a large difference in deviance indicates that either \n",
    "`DISTANCE` or `WIND` (or both) are significant, and it's unlikely that we would have seen as much reduction in deviance by chance alone. Now we calculate the significance of the fit parameters:\n",
    "\n",
    "```R\n",
    "# recall that pkfit resulted from calling glm on `PlacekickerData.csv`\n",
    "# with model specification GOOD ~ DISTANCE + WIND =>\n",
    "fitted <- unname(pkfit$fitted.values)\n",
    "actual <- df$GOOD\n",
    "ravg   <- mean(df$GOOD)  # 0.885614\n",
    "\n",
    "\n",
    "# function to compute the log-likelihood\n",
    "# y  => actual values (1/0)\n",
    "# pi => fitted probabilities\n",
    "ll <- function(y, pi) return(y*log(pi)+(1-y)*log(1-pi))\n",
    "\n",
    "# renaming `D` to  `residualDeviance` =>\n",
    "residualDeviance <- -2 * sum(ll(y=actual, pi=fitted))\n",
    "nullDeviance     <- -2 * sum(ll(y=actual, pi=ravg))\n",
    "devianceDiff     <- nullDeviance - residualDeviance\n",
    "\n",
    "# get degrees of freedom =>\n",
    "rd_df <- length(actual) - length(pkfit$coefficients)\n",
    "dd_df <- length(actual) - 1\n",
    "df    <- rd_df - nd_df # 2\n",
    "\n",
    "# pass devianceDiff and df to  pchisq function =>\n",
    "p_val <- pchisq(devianceDiff, df=df, lower.tail=FALSE)\n",
    "\n",
    "print(p_val)\n",
    "# returns 4.889108e-53\n",
    "```\n",
    "Such a small p-value indicates either `DISTANCE` or `WIND` (or both) are highly significant: Further analysis can be carried out to determine the source of deviance reduction. \n",
    "\n",
    "     \n",
    "### Deviance Residuals\n",
    "\n",
    "The *deviance residuals* are analogous to the residuals of a linear regression model. In linear regression, the residuals are the differences between the true outcome and the predicted output, whereas the *deviance residuals* are related to the log-likelihoods of having observed the true outcome, given the predicted probability of that outcome [2](#Footnotes:). The *deviance residuals* are obtained by taking the square root of the $i^{th}$ term of the deviance, with the sign selected to be the sign of $(actual - fitted)$:\n",
    "\n",
    "$$\n",
    "d_{i} = sign(y_{i} - \\hat{y}_{i}) \\sqrt{2\\Big(y_{i}Ln\\Big(\\frac{y_{i}}{\\hat{y}_{i}}\\Big) + (1 - y_{i})Ln\\Big(\\frac{1-y_{i}}{1-\\hat{y}_{i}}\\Big)\\Big)}\n",
    "$$\n",
    "\n",
    "\n",
    "We can take advantage of a simplification to the *deviance residuals* expression the same way we did with residual deviance: Multiply the sign of the difference between actual and fitted values by the square root of $-2$ times the log-likelihood of all data points, then return the quantile summary to compare with the *pkfit*'s quantiles:\n",
    "\n",
    "```R\n",
    "# recall that pkfit resulted from calling glm on `PlacekickerData.csv`\n",
    "# with model specification GOOD ~ DISTANCE + WIND =>\n",
    "fitted <- unname(pkfit$fitted.values)\n",
    "actual <- df$GOOD\n",
    "\n",
    "# function to compute the log-likelihood\n",
    "# y  => actual values (1/0)\n",
    "# pi => fitted probabilities\n",
    "ll <- function(y, pi) return(y*log(pi)+(1-y)*log(1-pi))\n",
    "\n",
    "# deviance residuals definition =>\n",
    "devianceResiduals <- function(y, pi) {\n",
    "    return(sign(y-pi)*sqrt(-2*ll(y, pi)))\n",
    "}\n",
    "\n",
    "\n",
    "devRes <- devianceResiduals(y=actual, pi=fitted)\n",
    "\n",
    "# call `summary` on devRes =>\n",
    "summary(devRes)\n",
    "\n",
    "# returns:\n",
    "    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n",
    "-2.7660  0.2353  0.2353  0.1490  0.3706  1.5910 \n",
    "```\n",
    "\n",
    "For comparison, the deviance residuals for `pkfit` were:\n",
    "\n",
    "```R\n",
    "Deviance Residuals: \n",
    "    Min       1Q   Median       3Q      Max  \n",
    "-2.7662   0.2353   0.2353   0.3706   1.5914  \n",
    "```\n",
    "\n",
    "### Akaike information criterion (AIC)\n",
    "\n",
    "A collection of candidate models can be compared, and the selection criteria may be to choose the model with the highest log-likelihood. However, the log-likelihood of a model will almost always increase with the addition of more variables, even if those variables are insignificant and do little to increase the model's predictive power. \n",
    "The *Akaike information criterion*, or *AIC*, is a penalized log-likelihood formula that charges a penalty for additional variables. It can be thought of as a measure of the relative quality of a model. When considering one or more models fit to the same dataset, *the preferred model is the one with the minimum AIC value*. The AIC formula is:\n",
    "\n",
    "$$\n",
    "AIC = -2l + 2q,\n",
    "$$\n",
    "\n",
    "where $l$ is the log-likelihood of the model of interest and $q$ is the number of parameters including the intercept term. Thew lower the AIC, the better the model.\n",
    "\n",
    "```R\n",
    "# recall that pkfit resulted from calling glm on `PlacekickerData.csv`\n",
    "# with model specification GOOD ~ DISTANCE + WIND =>\n",
    "fitted <- unname(pkfit$fitted.values)\n",
    "actual <- df$GOOD\n",
    "\n",
    "# function to compute the log-likelihood\n",
    "# y  => actual values (1/0)\n",
    "# pi => fitted probabilities\n",
    "ll <- function(y, pi) return(y*log(pi)+(1-y)*log(1-pi))\n",
    "\n",
    "# capture number of model coefficients =>\n",
    "nbr_coeffs <- length(pkfit$coefficients) # 3\n",
    "\n",
    "AIC <- -2*ll(y=actual, pi=fitted) + 2*nbr_coeffs\n",
    " \n",
    "print(AIC)\n",
    "\n",
    "# returns 778.5335\n",
    "```\n",
    "\n",
    "AIC for *pkfit* was $778.53$.\n",
    "\n",
    "\n",
    "  \n",
    "### Pseudo R-squared\n",
    "\n",
    "The *pseudo R-squared*, or *Generalized R-squared* measures how much deviance is explained by the model. It's range is between 0 and 1, with models having a value closer to 1 explaining more of the deviance than a model having a value closer to 0. \n",
    "The formula for pseudo R-squared is:\n",
    "\n",
    "$$\n",
    "R^{2}_{pseudo} = \\frac {l_{min} - l}{l_{min}} = 1 - \\frac {deviance_{residual}}{deviance_{null}}\n",
    "$$\n",
    "\n",
    "where $l_{min}$ is the log-likelihood for the intercept-only model, and $l$ is the log-likelihood associated with the model in question. Using our null and residual deviances from earlier:\n",
    "\n",
    "```R\n",
    "nullDeviance     <- pkfit$null.deviance # 1013.426\n",
    "residualDeviance <- pkfit$deviance      # 772.5335\n",
    "pseudoRSquared   <- 1 - (residualDeviance/nullDeviance)\n",
    "\n",
    "print(pseudoRSquared)\n",
    "\n",
    "# returns 0.2377013\n",
    "```\n",
    "\n",
    "Our `GOOD ~ DISTANCE + WIND` model only explains ~24% of the deviance, which means that we haven't yet identified all the factors that predict whether or not a placekick is made. We should now go back and test the quality of the other explanatory variables, and check if another combination of explanatory variables has similiar goodness of fit characteristics but with a better pseudo R-squared.  \n",
    "\n",
    "\n",
    "### Conclusion\n",
    "In this post, we walked through some of the methods used to assess goodness of fit and significance of explanatory variables for a Logistic Regression model, and demonstrated how to calculate many of the important summary statistics output by R's `glm` function. We also discusses some of the simplifications that can be made in calculating deviance and deviance residuals when modeling a dataset with a dichotomous response. In future posts, I plan to cover model assessment techniques for other members of the exponential family. Until then, happy coding!\n",
    "\n",
    "\n",
    "\n",
    "### Footnotes:\n",
    "1. Bilder, Christopher, Thomas M. Loughin (2015). *Analysis of Categorical Data in R*. Taylor & Francis Group, New York    \n",
    "2. Mount, John, Nina Zumel (2014). *Practical Data Science in R*. Manning Publications, New York     \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
