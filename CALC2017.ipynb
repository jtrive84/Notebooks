{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python PDF Harvester in $\\le$ 25 LOC!\n",
    "\n",
    "<br><br>\n",
    "\n",
    "Python is an excellent programming language for web-related tasks. \n",
    "\n",
    "In the following demo, we develop a utility that performs the following steps:     \n",
    "<br>\n",
    "1. **Capture the underlying HTML for a given webpage of interest** (henceforth the *wpoi*)   \n",
    "<br>   \n",
    "2. **Parse the HTML, and extract all references to PDF documents with associated URLs**     \n",
    "<br>      \n",
    "3. **For each PDF URL reference, retrieve the source document and write the contents to file**    \n",
    "<br>    \n",
    "\n",
    "    \n",
    "\n",
    "## Step I: Capture Underlying HTML\n",
    "\n",
    "In order to get the demo to run correctly, you'll need to provide details of our (CNA's) proxy server upon requesting content from our wpoi's. \n",
    "\n",
    "<br>\n",
    "**Note that this step is required only because we're accessing the internet from within the Enterprise. If you were to repeat the demo at home, skip the proxy-server intialization step \n",
    "entirely.**\n",
    "<br>  \n",
    "\n",
    "\n",
    "This is necessary because every internet-bound web request made throughout the Enterprise is first routed through an intermediate proxy server prior to being passed along to the internet at large. \n",
    "\n",
    "**Case in point:**\n",
    "\n",
    "Follow the link to *WhatsMyIP.org*: [http://www.whatsmyip.org/](http://www.whatsmyip.org/) \n",
    "<br>  \n",
    "Everyone here will have the same IP: **159.10.134.170**. \n",
    "\n",
    "This is the public-facing IP address of our proxy server. \n",
    "\n",
    "As a security measure, in order to make internet requests, users need to provide their username and password, along with the address and port of our proxy server. \n",
    "\n",
    "Note that your authentication details are not \"sent out\" to the internet: They're used only so that the proxy server can verify that the request is originating from an authorized user. \n",
    "        \n",
    "This authentication happens behind the scenes automatically with every *browser-based* internet submission, but when submitting web requests via programming language, this step needs to be performed manually. \n",
    "     \n",
    "To illustrate, from Internet Explorer, go to *Tools > Internet Options > Connections > LAN Settings*. You'll find:\n",
    "    \n",
    "*  Address: **proxy.cna.com**\n",
    "*  Port: **8080**\n",
    "\n",
    "The proxy address and port are used in conjunction with your CID and password whenever you perform a web search. \n",
    "\n",
    "\n",
    "Our first step will be to create a dictionary with two entries: one for **http** and another for **https** requests. The format is as follows:\n",
    "\n",
    "\n",
    "```\n",
    "http://<CID>:<password>@proxy.cna.com:8080\n",
    "https://<CID>:<password>@proxy.cna.com:8080\n",
    "```\n",
    "<br>  \n",
    "For example, if a user's CID is **`CCC3313`** with password **`P@ssword9999`**, their `proxies` dict would look like:\n",
    "\n",
    "\n",
    "```python\n",
    "proxies = {'http': 'http://CCC3313:P@ssword9999@proxy.cna.com:8080',\n",
    "           'https': 'https://CCC3313:P@ssword9999@proxy.cna.com:8080'}\n",
    "```\n",
    "\n",
    "A few things to note:\n",
    "\n",
    "* In Python parlance, `http/https` are called dictionary \"keys\", and the associated proxy strings are called \"values\". Dictionaries are comprised of *key-value* pairs.    \n",
    "<br>   \n",
    "\n",
    "* In the `proxies` dict, the keys and values are surrounded by quotes (either single or double; Python does not make a distinction). Both the keys and values are string datatypes in the `proxies` dict.     \n",
    "<br>\n",
    "\n",
    "* The first key-value pair is separated from the second by a comma: This is also the convention when dealing with dicts containing more than two items. Keys are separated from values with a colon, key-value pairs are separated from each other by commas.    \n",
    "<br>\n",
    "\n",
    "\n",
    "The library that facilitates communication between Python and the wpoi is [`requests`](http://docs.python-requests.org/en/master/). It exposes a simple, intuitive interface that works right out of the box (\"batteries included\"). To capture the wpoi's underlying HTML, we only need call:\n",
    "\n",
    "```\n",
    "requests.get('URL', proxies=proxies).text\n",
    "```\n",
    "\n",
    "Where `URL` is a string representing the URL of interest. **`requests.get`** returns an object, and by including the `text` suffix, we're requesting the the wpoi's content be returned as plain text to allow for parsing with regular expressions in the next step. \n",
    "\n",
    "<br><br>\n",
    "What follows is the Python code that corresponds to **`Step I`** of our PDF Harvester demo:   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# PDF Harvester I of III: Retrieve HTML as plain text               |\n",
    "# ===================================================================\n",
    "import requests\n",
    "from pprint import pprint\n",
    "\n",
    "# *** uncomment and replace proxy string with your authentication details ***\n",
    "# proxies = {\n",
    "#         'http' :\"http://CCC3313:P@ssword9999@proxy.cna.com:8080\",\n",
    "#         'https':\"https://CCC3313:P@ssword9999@proxy.cna.com:8080\"\n",
    "#         }\n",
    "\n",
    "\n",
    "# specify URL for sample wpoi =>\n",
    "URL = \"https://en.wikipedia.org/wiki/Loss_reserving\"\n",
    "\n",
    "# instruct request object to return HTML as plain text =>\n",
    "html = requests.get(URL, proxies=proxies).text\n",
    "\n",
    "# print raw html =>\n",
    "pprint(html, width=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>   \n",
    "We've captured the HTML. Next we need to identify and extract references to PDF documents in the form of URLs. For this step, we'll leverage the power of regular expressions, available in the Python Standard Library within the **`re`** module.    \n",
    "<br><br>      \n",
    "\n",
    "\n",
    "\n",
    "## Step II: Extract PDF URLs from HTML\n",
    "\n",
    "**Regular Expressions** are a sequence of characters that define a search pattern.\n",
    "\n",
    "Example from SQL:\n",
    "\n",
    "```sql\n",
    "SELECT * FROM TABLE WHERE FIELD LIKE ‘%FIRE%’;\n",
    "```\n",
    "\n",
    "This query returns all records from **TABLE** where **FIELD** contains\n",
    "‘FIRE’ along with any additional leading or trailing characters:\n",
    "\n",
    "* BLD_FIRE\n",
    "* CONT_FIRE\n",
    "* BINT_FIRE\n",
    "* “Fire on the Mountain”\n",
    "* “Jump in the Fire”\n",
    "\n",
    "\n",
    "*`LIKE`* is useful for capturing string literals from a larger corpus of text, but cannot be used to match instances in which the target string follows a fixed format but with variable content:\n",
    "\n",
    "```\n",
    "\"Match 3 characters followed by 7 integers followed by 3 vowels\"\n",
    "```\n",
    "<br>    \n",
    "\n",
    "\n",
    "Put simply, Regular Expressions use special symbols to represent collections of characters.\n",
    "A few of the most common symbols are:\n",
    "\n",
    "*  **\\d** matches any decimal digit [0-9]   \n",
    "<br>     \n",
    "*  **\\D** matches any non-digit character [^0-9]    \n",
    "<br>    \n",
    "*  **\\s** matches any whitespace character [\\t\\n\\r\\f\\v]    \n",
    "<br>    \n",
    "*  **\\S** matches any non-whitespace character [^\\t\\n\\r\\f\\v]   \n",
    "<br>    \n",
    "*  **\\w** matches any alphanumeric character [a-zA-Z0-9_]   \n",
    "<br>    \n",
    "*  **\\W** matches any non-alphanumeric character [^a-zA-Z0-9_]   \n",
    "<br>    \n",
    "*  **+** Matches 1 or more of preceding symbol or literal\n",
    "<br>    \n",
    "*  ***** matches 0 or more of preceding symbol or literal   \n",
    "<br>    \n",
    "*  **^** matches at beginning of line   \n",
    "<br>\n",
    "*  **$** matches at end of line     \n",
    "<br>\n",
    "*  **?** affixed to the end of a symbol or literal changes match type to non-greedy from greedy.     \n",
    "<br>    \n",
    "<br>     \n",
    "\n",
    "\n",
    "\n",
    "The last three symbols, `+`, `*` and `?`, are used in conjunction with other character classes:\n",
    "\n",
    "*  **\\d+** will match 1 or more consecutive decimal digits [0-9]    \n",
    "<br>   \n",
    "*  **\\w*** will match 0 or more alphanumeric characters [a-zA-Z0-9_]      \n",
    "<br>\n",
    "*  [Pythex](http://pythex.org/) example with `<Alpha><Beta><Gamma><Delta>` and `^<Alpha.*?>`\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "### Lookahead Assertion\n",
    "\n",
    "\n",
    "\n",
    "Another regular expression utility we'll leverage is the *lookahead assertion*:\n",
    "\n",
    "*  **(?=char_sequence)** \t\n",
    "\n",
    "\n",
    "The *lookahead* assertion matches without consuming. It can be used to tell a regular expression where to start matching, then use additional symbols to capture content.    \n",
    "<br>     \n",
    "   \n",
    "Consider the Beatles discography:    \n",
    "\n",
    "```\n",
    "Please Please Me\n",
    "With the Beatles\n",
    "A Hard Day's Night\n",
    "Beatles For Sale\n",
    "Help!\n",
    "Rubber Soul\n",
    "Revolver\n",
    "Sgt. Pepper's Lonely Hearts Club Band\n",
    "Magical Mystery Tour\n",
    "The White Album\n",
    "Yellow Submarine\n",
    "Abbey Road\n",
    "Let It Be\n",
    "```\n",
    "\n",
    "In order to identify album names starting with **`R`**, we'd use the following lookahead assertion, along with `^` and `.+`:\n",
    "\n",
    "```\n",
    "^(?=R).+\n",
    "```\n",
    "\n",
    "This regular expression highlights any album names beginning with `\"R\"`.   \n",
    "<br>\n",
    "To *extract* the matching album names, we need to surround the text of interest with parens. In Python, this is called a *regular expression capture group*. The above regular expression is modified only by the inclusion of parens surrounding **`.+`**:\n",
    "\n",
    "```\n",
    "^(?=R)(.+)\n",
    "\n",
    "```\n",
    "   \n",
    "<br>  \n",
    "[Verify using Pythex](http://pythex.org/)   \n",
    "<br>        \n",
    "       \n",
    "    \n",
    " \n",
    "We now have everything we need to extract PDF URLs from the HTML captured in Step I.\n",
    "\n",
    "\n",
    "#### Identifying PDF URL's\n",
    " \n",
    "We can take advantage of a few insights to help streamline the compilation of our regular expression. \n",
    "\n",
    "We'll use the following link as a reference: [https://en.wikipedia.org/wiki/Loss_reserving](https://en.wikipedia.org/wiki/Loss_reserving).\n",
    "\n",
    "<br>   \n",
    "1.  From any browser, pressing **Ctrl + U** will display the underlying HTML for the current webpage. This can be useful for inspecting the underlying HTML and searching for text patterns.       \n",
    "<br>             \n",
    "2.  Valid PDF URLs will in all cases be embedded within an **href** tag.   \n",
    "<br>      \n",
    "3.  Valid PDF URLs will in all cases be preceded by **http** or **https**.    \n",
    "<br>   \n",
    "4.  Valid PDF URLs will in all cases be enclosed by a trailing **>**.        \n",
    "<br>       \n",
    "5.  Valid PDF URLs cannot contain whitespace.        \n",
    "\n",
    "<br><br>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The following snippet is characteristic of the HTML the harvester will be required to parse:\n",
    "\n",
    "\n",
    "```html\n",
    "<html>\n",
    "<span class=\"reference-text\"><a class=\"external\" href=\"http://Public/GuelahPapyrus.pdf\">\n",
    "    <a><span><li>< href=\"http://ocw.mit.edu/courses/StatisticalModeling.pdf\"></a></span></li>\n",
    "    <a href=\"#cite_ref-3\">^</a></b></span> <span class=\"reference-text\">\n",
    "    <a class=\"internal\" href=\"https://The/Watchful/Horsemasters.pdf\">\n",
    "        <a><i>href=\"http://arxiv.org/pdf/1311.1704v3.png\"><i>Scalable Recommendation</i></a>\n",
    "        <a href=\"#cite_ref-Papoulis_5-0\"><sup><i><b>a</b></i></sup>\n",
    "<href>masterdownload%255C2009519932327055475115776.pdf&amp;rft_id=info%3Adoi%2F10.1016%2Fj</href>\n",
    "</html>\n",
    "```\n",
    "\n",
    "From this HTML, our regular expression needs to be able to extract:\n",
    "\n",
    "*  **`http://Public/GuelahPapyrus.pdf`**        \n",
    "<br>   \n",
    "*  **`http://ocw.mit.edu/courses/StatisticalModeling.pdf`**        \n",
    "<br>\n",
    "*  **`https://The/Watchful/Horsemasters.pdf`**    \n",
    "<br>\n",
    "Using only what we've covered so far.     \n",
    "<br><br>      \n",
    "\n",
    "Load the sample HTML in [Pythex](http://pythex.org/), and let's create a regular expression to extract the URLs!\n",
    "\n",
    "\n",
    "\n",
    "We now extend our Step I code to include logic to extract PDF URLs from retrieved HTML. The one additional library leveraged in Step II is `re`, which is Python's regular expression module. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# PDF Harvester II of III: Extract PDF URL's from HTML              |\n",
    "# ===================================================================\n",
    "import requests\n",
    "import re\n",
    "\n",
    "# *** uncomment and replace proxy string with your authentication details ***\n",
    "# proxies = {\n",
    "#         'http' :\"http://CCC3313:P@ssword9999@proxy.cna.com:8080\",\n",
    "#         'https':\"https://CCC3313:P@ssword9999@proxy.cna.com:8080\"\n",
    "#         }\n",
    "\n",
    "\n",
    "# specify URL for webpage of interest =>\n",
    "URL = \"https://en.wikipedia.org/wiki/Loss_reserving\"\n",
    "\n",
    "# instruct request object to return HTML as plain text =>\n",
    "html = requests.get(URL, proxies=proxies).text\n",
    "\n",
    "# search html and compile PDF URL's =>\n",
    "pdf_urls = re.findall(r\"(?=href=).*(https?://\\S+.pdf).*?>\", html)\n",
    "\n",
    "# display content of pdf_urls list => \n",
    "print(pdf_urls)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "Note that the regular expression is preceded with an **`r`** when passed to `re.findall`. This instructs the Python virtual machine to interpret what follows as a raw string and to ignore all escape codes.\n",
    "\n",
    "`re.findall` returns a list of all matches extracted from the source text. In our case, it returns a list of URLs.\n",
    "\n",
    "Finally, we need to retrieve the content associated with a given PDF and write it to file locally. \n",
    "We introduce another module from the Python Standard Library, `os.path`, which facilitates the partitioning of absolute filepaths into components in order to retain filenames when saving to disk.    \n",
    "For example, consider the following well-formed URL:\n",
    "\n",
    "\n",
    "```\n",
    "\"http://Statistical_Modeling/Fall_2017/Lectures/Lecture11.pdf\"\n",
    "```\n",
    "\n",
    "To capture `Lecture11.pdf`, we pass the absolute URL to `os.path.split`, which returns\n",
    "a tuple of everything preceeding the filename as the first element, along with the filename and extension as the second element:\n",
    "\n",
    "```python\n",
    ">>> import os.path\n",
    ">>> url = \"http://Statistical_Modeling/Fall_2017/Lectures/Lecture11.pdf\"\n",
    ">>> os.path.split(url)\n",
    "('http://Statistical_Modeling/Fall_2017/Lectures', 'Lecture11.pdf')\n",
    "```\n",
    "\n",
    "Therefore, we can capture the filename and extension by calling `os.path.split(url)`, and using Python's index notation to specify the element at the second position in the tuple, **`os.path.split(url)[1]`**.\n",
    "\n",
    "\n",
    "## Step III: Capture PDF Content and Write to File\n",
    "\n",
    "This step differs from the initial HTML retrieval in that we need to request the content as bytes, not text. By calling `requests.get(url, proxies=proxies).content`, we're accessing the raw bytes that comprise the PDF, then writing those bytes directly to file. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# PDF Harvester III of III: Write PDF(s) to file                    |\n",
    "# ===================================================================\n",
    "import requests\n",
    "import re\n",
    "import os\n",
    "import os.path\n",
    "\n",
    "# *** uncomment and replace proxy string with your authentication details ***\n",
    "# proxies = {\n",
    "#         'http' :\"http://CCC3313:P@ssword9999@proxy.cna.com:8080\",\n",
    "#         'https':\"https://CCC3313:P@ssword9999@proxy.cna.com:8080\"\n",
    "#         }\n",
    "\n",
    "\n",
    "# specify URL for webpage of interest =>\n",
    "# URL = \"https://en.wikipedia.org/wiki/Loss_reserving\"            #1 PDFs\n",
    "# URL = \"https://en.wikipedia.org/wiki/Law_of_total_variance\"     #2 PDFs\n",
    "# URL = \"https://en.wikipedia.org/wiki/Kernel_density_estimation\" #3 PDFs\n",
    "# URL = \"https://en.wikipedia.org/wiki/Logistic_regression\"       #4 PDFs\n",
    "# URL = \"https://en.wikipedia.org/wiki/Exponential_distribution\"  #6 PDFs\n",
    "# URL = \"https://en.wikipedia.org/wiki/Gamma_distribution\"        #8 PDFs\n",
    "# URL = \"https://en.wikipedia.org/wiki/Naive_Bayes_classifier\"    #8 PDFs\n",
    "\n",
    "\n",
    "# instruct requests object to return HTML as plain text =>\n",
    "html = requests.get(URL, proxies=proxies).text\n",
    "\n",
    "# search html and compile PDF URLs =>\n",
    "pdf_urls = re.findall(r\"(?=href=).*(https?://\\S+.pdf).*?>\", html)\n",
    "\n",
    "# display extracted PDF URLs =>\n",
    "for i in pdf_urls: print(i)\n",
    "\n",
    "\n",
    "# set working directory to desired location =>\n",
    "os.chdir(\"C:\\\\Users\\\\cac9159\\\\Downloads\\\\\")\n",
    "\n",
    "# request PDF content and write to file for all entries\n",
    "# in pdf_urls =>\n",
    "for pdf in pdf_urls:\n",
    "    \n",
    "    # get filename from url =>\n",
    "    pdfname = os.path.split(pdf)[1]\n",
    "    \n",
    "    print(\"Saving {}...\".format(pdfname))\n",
    "    \n",
    "    # get retrieved html as `content` =>\n",
    "    r = requests.get(pdf, proxies=proxies).content\n",
    "    \n",
    "    try:\n",
    "        # write content of r to file, using same name as pdf =>\n",
    "        with open(pdfname, \"wb\") as f: f.write(r)\n",
    "            \n",
    "    except:\n",
    "        print(\"Unable to download {}.\".format(pdfname))\n",
    "        continue\n",
    "        \n",
    "print(\"\\nProcessing complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we surround `with open(pdfname, \"wb\")...` with a `try-except` block: This is how exception handling is implemented in Python, and handles situations that prevent the PDF from being downloaded, such as empty redirects, broken links or an invalid server-side SSL configuration to name a few. \n",
    "\n",
    "Finally, we present the PDF Harvester with the commands collected into a function and with comments stripped away. Remember, in Python, **local scope always executes faster than global scope**, so there's a legitimate performance incentive for encapsulating logic within functions (other than it being just a best practice). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# PDF Harvester: Functional Representation                          |\n",
    "# ===================================================================\n",
    "import requests\n",
    "import re\n",
    "import os\n",
    "import os.path\n",
    "\n",
    "# proxies = {'http' :\"http://CCC3313:P@ssword9999@proxy.cna.com:8080\",\n",
    "#            'https':\"https://CCC3313:P@ssword9999@proxy.cna.com:8080\"}\n",
    "\n",
    "\n",
    "def pdf_harvester(url, proxies, loc=None):\n",
    "    \"\"\"\n",
    "    Retrieve url's html and extract references to PDFs.\n",
    "    Download PDFs, writting to `loc`. If `loc` is None, \n",
    "    save to current working directory.\n",
    "    \"\"\"\n",
    "    print(\"Harvesting PDFs from => {}\\n\".format(url))\n",
    "    os.chdir(os.getcwd() if loc is None else loc)\n",
    "    html     = requests.get(URL, proxies=proxies).text\n",
    "    pdf_urls = re.findall(r\"(?=href=).*(https?://\\S+.pdf).*?>\", html)\n",
    "    \n",
    "    for pdf in pdf_urls:\n",
    "    \n",
    "        pdfname = os.path.split(pdf)[1]\n",
    "        r       = requests.get(pdf, proxies=proxies).content\n",
    "        \n",
    "        try:\n",
    "            print(\"Downloading {}...\".format(pdfname))\n",
    "            with open(pdfname, \"wb\") as f: f.write(r)   \n",
    "                \n",
    "        except:\n",
    "            print(\"Unable to download {}.\".format(pdfname))\n",
    "            continue\n",
    "            \n",
    "    print(\"\\nProcessing complete.\")\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "# example calling `pdf_harvester` =>\n",
    "URL = \"https://en.wikipedia.org/wiki/Poisson_point_process\"\n",
    "pdf_harvester(URL, proxies, loc=\"C:\\\\Users\\\\cac9159\\\\Downloads\\\\\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDF Harvester Potential Future Enhancements\n",
    "\n",
    "1. Include an additional function argument that specifies what type of file to search and extract. Any \n",
    "   file that can be downloaded from a webpage must be preceded by an `href` in the HTML markup. Therefore, the `pdf_harvester` can be modified to download any type of file.    \n",
    "<br>    \n",
    "2. Leverage the Standard Library's `argparse` module to convert the `pdf_harvester` into a command line script that accepts and parses command line arguments. Simply pass the script name along with the URL of interest, and `pdf_harvester` will download and save PDFs without ever having to open the source file in a graphical interface.     \n",
    "<br>     \n",
    "3.  Note that `pdf_harvester` isn't limited to just downloading files: Once the html is retrieved, it \n",
    "can be parsed to extract any content you desire. For example, imagine retrieving financial information for a particular stock during trading hours from Yahoo Finance. Simply pass the URL of interest to a modified `pdf_harvester`, and search the retrieved HTML for the stock price at the moment of retrieval. Then, once you've identified a consistent text pattern in the vicinity of the stock quote markup, you can retrieve the HTML on a periodic basis for whatever collection of stocks you're interested in tracking. `pdf_harvester` would be much simplier for this enhancement than the original implementation: You would only need to remove all commands from the beginning of the `for` loop to the end of the function.\n",
    "<br>\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
