{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Introducing scikit-learn\n",
    "<br>   \n",
    "Author: James D. Triveri    \n",
    "<br>\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**scikit-learn** is a popular, full-featured machine learning library written for Python. \n",
    "The API is remarkably well designed. The library exposes and makes regular use of the following objects: \n",
    "<br>     \n",
    "  * **Estimators**: Any object that can estimate a set parameters based on a dataset\n",
    "    is called an estimator. The estimation itself is\n",
    "    performed by the `fit()` method, and it takes only a dataset as a parameter (or\n",
    "    two for supervised learning algorithms, the second dataset contains the\n",
    "    labels). Any other parameter needed to guide the estimation process is con‐\n",
    "    sidered a hyperparameter.  \n",
    "<br>    \n",
    "  * **Transformers**: Some estimators can also transform a dataset. The\n",
    "    transformation is facilitated by the `transform()` method, which returns the \n",
    "    transformed dataset. The transformation generally relies on the learned \n",
    "    parameters, as is the case for an imputer.   \n",
    "<br>         \n",
    "  * **Predictors**: Some estimators are capable of making predictions given a\n",
    "    dataset. A predictor has a `predict()` method that takes a dataset of new \n",
    "    instances and returns a dataset of corresponding predictions. Predictors also \n",
    "    have a `score()` method that measures the quality of the predictions given\n",
    "    a test set (and the corresponding labels in the case of supervised learning\n",
    "    algorithms).     \n",
    "<br>   \n",
    "\n",
    "\n",
    "\n",
    "To implement a model in scikit-learn:\n",
    "\n",
    "* **Select a model appropriate for the task at hand** (see below)        \n",
    "<br>   \n",
    "* **Pre-process explanatory data** (scale variables, impute missing data, encode categorical variables)     \n",
    "<br>\n",
    "* **Instantiate model**        \n",
    "<br>        \n",
    "* **Fit model to training data**            \n",
    "<br>   \n",
    "* **Tune hyperparameters via cross-validation**       \n",
    "<br>    \n",
    "* **Predict classes on test/holdout data**    \n",
    "<br>    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![sklearn cheat-sheet](ml_map.png)\n",
    "\n",
    "\n",
    "<br>  \n",
    "    \n",
    "[Link to cheat-sheet](http://peekaboo-vision.blogspot.de/2013/01/machine-learning-cheat-sheet-for-scikit.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "Preprocessing in sklearn is primarily concerned with:\n",
    "\n",
    "* **Encoding**: Encode string features as ints; 'One-Hot' encode features with more than 2 values.   \n",
    " <br>  \n",
    "* **Scaling**: Standardize explanatory variables. The two most common techniques are:    \n",
    "\n",
    "    * `StandardScaler`: Standardize features by removing the mean and scaling to unit variance.   \n",
    "    <br>     \n",
    "    * `MinMaxScaler`: Scales and translates each feature individually such that it is in the \n",
    "      given range on the training set, i.e. between 0 and 1.   \n",
    " <br> \n",
    "* **Imputing**: Replace missing values with the mean, median or most frequent value of \n",
    "  the corresponding feature.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Preprocessing Example =>\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'COL1'    : [215.43,np.nan,212.08,169.23,428.43,112.15,np.nan,338.20,192.25,213.40],\n",
    "    'COL2'    : ['A','B','A','B','B','A','B','A','B','B'],\n",
    "    'COL3'    : [ np.nan,2.39476,1.78377,0.24551,np.nan,-0.99175,-0.01066,-0.88275,0.21074,-0.5943],\n",
    "    'COL4'    : [5501.75,9465.95,9544.02,11564.05,4984.09, 4467.97,np.nan,26996.26,np.nan,6763.45],\n",
    "    'COL5'    : ['M', 'F', 'F', 'F', 'M', 'F', 'M', 'M', 'F', 'M'],\n",
    "    'RESPONSE': ['Y', 'Y', 'N', 'N', 'Y', 'N', 'Y', 'Y', 'N', 'Y']\n",
    "    })\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# [1] Impute missing values =>\n",
    "\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "# `strategy` can be one of 'mean', 'median' or 'most_frequent'\n",
    "imp = Imputer(missing_values=np.nan, strategy='mean')\n",
    "\n",
    "df[['COL1', 'COL3', 'COL4']] = imp.fit_transform(df[['COL1', 'COL3', 'COL4']])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# [2] Encode categorical features as integers =>\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "df['COL2']     = le.fit_transform(df['COL2'])\n",
    "df['COL5']     = le.fit_transform(df['COL5'])\n",
    "df['RESPONSE'] = le.fit_transform(df['RESPONSE'])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# [3] Scale all features with StandardScaler =>\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sclr = StandardScaler()\n",
    "\n",
    "df[['COL1','COL3','COL4']] = sclr.fit_transform(df[['COL1','COL3','COL4']])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# design matrix must be fed to classifier as a numpy array =>\n",
    "\n",
    "# `as_matrix` coerces DataFrame to numpy array\n",
    "X = df.drop('RESPONSE', axis=1, inplace=True).as_matrix()\n",
    "y = df['RESPONSE'].values\n",
    "\n",
    "# ready to pass X, y to classifier..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## scikit-learn Classifiers\n",
    "\n",
    "[List of available sklearn models](http://scikit-learn.org/stable/user_guide.html)\n",
    "\n",
    "\n",
    "\n",
    "We'll discuss the following classifiers:\n",
    "\n",
    "* **Gaussian Naive Bayes**\n",
    "* **k-Nearest Neighbors**\n",
    "* **Logistic Regression**\n",
    "* **Support Vector Machines**\n",
    "* **Voting Classifiers** (Ensemble MEthod)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Gaussian Naive Bayes\n",
    "\n",
    "*Naive Bayes* methods are a set of supervised learning algorithms based on applying Bayes’ Theorem with the “naive” assumption of independence between every pair of features.\n",
    "The Naive Bayes classifier makes two strong assumptions:    \n",
    "<br>    \n",
    "\n",
    "1.  **The value of a particular feature is independent of the value of any other feature, given the class variable.**   \n",
    "<br>\n",
    "2.  **The set of features associated with an unclassified instance are assumed to follow a normal distribution.**      \n",
    "\n",
    "<br>\n",
    "\n",
    "**To create a Gaussian Naive Bayes Classifier (without scikit-learn):**\n",
    "\n",
    "1. Ensure all explanatory variables are continuous: If the dataset contains categorical features, look into the Bernoulli or Multinomial form of Naive Bayes.  \n",
    "<br>    \n",
    "2. For each explanatory variable, calculate the maximum likelihood estimate of the mean and variance for each class.   \n",
    "<br>     \n",
    "3. To classify a new instance, calculate the posterior probability for each class. There will be as many posterior probabilities per unclassified instance as there are distinct classes.    \n",
    "<br>    \n",
    "4. The new instance will be classified based on the class with the greatest posterior probability.           \n",
    "<br>   \n",
    "\n",
    "**Example:**\n",
    "\n",
    "Consider a sample dataset representing business school admissions:\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "|  ID        |  GPA  |  GMAT   |   ADMITTED_IND   |\n",
    "|:----------:|:-----:|:-------:|:------------:|\n",
    "| 000000001  |  3.14 |\t473\t   |1             |\n",
    "| 000000002  |  3.22 |\t482\t   |1             |\n",
    "| 000000003  |  2.96 |\t596\t   |1             |\n",
    "| 000000004  |  3.28 |\t523\t   |1             | \n",
    "| 000000005  |  2.72 |\t399\t   |0             |\n",
    "| 000000006  |  2.85 |\t381\t   |0             |\n",
    "| 000000007  |  2.51 |\t458\t   |0             | \n",
    "| 000000008  |  2.36 |\t399\t   |0             |\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "We have two additional instances that will be used to test the classifier:\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "|  ID        |  GPA  |  GMAT   |   ADMITTED_IND   |\n",
    "|:----------:|:-----:|:-------:|:------------:|\n",
    "| 000000009  |  2.90 |  384    |0             |\n",
    "| 000000010  |  3.40 |  431    |1             |\n",
    "<br>\n",
    "<br>\n",
    "For each feature, we calculate the mean and variance for admitted and not-admitted:  \n",
    "<br> \n",
    "\n",
    "|  ADMITTED_IND  |$\\mu_{GPA}$ |$\\sigma^{2}_{GPA}$|$\\mu_{GMAT}$|$\\sigma^{2}_{GMAT}$|\n",
    "|:----------:|:----------:|:----------------:|:----------:|:-----------------:|\n",
    "|   1/yes    |  3.150     |  0.0193          |  518.50    |  3143.00          |        \n",
    "|   0/no     |  2.610     |  0.0474          |  409.25    |  1128.25          |\n",
    "<br>  \n",
    "\n",
    "In the sample dataset, we have equiprobable priors (since $P(admit) = P(!admit) = .5$). However, the prior probabilities need not be derived from the dataset of interest. They can be based on external data sources (such as admissions from prior years). \n",
    "\n",
    "<br> \n",
    "Recall the general form of Bayes' Theorem:\n",
    "\n",
    "$$\n",
    "P(A|B) = \\frac{P(B|A)P(A)}{P(B)}\n",
    "$$\n",
    "<br> \n",
    "<br>\n",
    "\n",
    "The posterior probability for admitted is given by:   \n",
    "<br>    \n",
    "$$\n",
    "P(admit|data) = \\frac {P(admit)P(GPA|admit)P(GMAT|admit)}{P(data)},\n",
    "$$\n",
    "<br>\n",
    "<br> \n",
    "\n",
    "and for not-admitted:    \n",
    "<br>\n",
    "$$\n",
    "p(!admit|data) = \\frac {P(!admit)P(GPA|!admit)P(GMAT|!admit)}{P(data)},\n",
    "$$\n",
    "<br>  \n",
    "Where: \n",
    "   \n",
    "*  $P(admit)/P(!admit)$ represents the prior probability, $.50$ in this example.      \n",
    "<br>     \n",
    "*  $P(GPA|admit)P(GMAT|admit)$ represents the likelihood. We assume zero correlation between $GPA$ and $GMAT$ via the first assumption of Naive Bayes.     \n",
    "<br>     \n",
    "*  *data* is a stand-in for $GMAT$ and $GPA$ for a given instance.     \n",
    "<br>     \n",
    "\n",
    "The second assumption of Naive Bayes is that all explanatory variables follow a normal distribution. Thus, $P(GMAT∣admitted)$ is calculated by passing the observation's $GMAT$ score and $GPA$ into the associated normal density function, parameterized by the corresponding estimates of mean and variance determined above.  \n",
    "<br>    \n",
    "For the admitted class:\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "P(GMAT∣admit) &= \\frac {1} {\\sqrt{2 \\pi \\sigma^{2}_{GMAT|admit}}} exp\\Big({-\\frac {(GMAT - \\mu_{GMAT|admit})^{2}}{2\\sigma^{2}_{GMAT|admit}}}\\Big)\\\\ \\\\\n",
    "P(GPA∣admit) &= \\frac {1} {\\sqrt{2 \\pi \\sigma^{2}_{GPA|admit}}} exp\\Big({-\\frac {(GPA - \\mu_{GPA|admit})^{2}}{2\\sigma^{2}_{GPA|admit}}}\\Big)\n",
    "\\end{align*}\n",
    "$$\n",
    "<br>       \n",
    "\n",
    "For not-admitted:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "P(GMAT∣!admit) &= \\frac {1} {\\sqrt{2 \\pi \\sigma^{2}_{GMAT|!admit}}} exp\\Big({-\\frac {(GMAT - \\mu_{GMAT|!admit})^{2}}{2\\sigma^{2}_{GMAT|!admit}}}\\Big) \\\\ \\\\\n",
    "P(GPA∣!admit) &= \\frac {1} {\\sqrt{2 \\pi \\sigma^{2}_{GPA|!admit}}} exp\\Big({-\\frac {(GPA - \\mu_{GPA|!admit})^{2}}{2\\sigma^{2}_{GPA|!admit}}}\\Big)\n",
    "\\end{align*}\n",
    "$$\n",
    "<br>  \n",
    "<br>  \n",
    "\n",
    "### Classifying Instances\n",
    "\n",
    "Recall our test observations:\n",
    "<br>\n",
    "\n",
    "|  ID        |  GPA  |  GMAT   |   ADMITTED_IND   |\n",
    "|:----------:|:-----:|:-------:|:------------:|\n",
    "| 000000009  |  2.90 |  384    |0             |\n",
    "| 000000010  |  3.40 |  431    |1             |\n",
    "<br>\n",
    "<br>\n",
    "We calculate the admitted and not-admitted posterior for each instance: The observation will be classified as admitted/1 or not-admitted/0 based on the class with the greatest posterior probability.   \n",
    "<br>       \n",
    "\n",
    "### For **ID=000000009**:  \n",
    "<br>  \n",
    "\n",
    "GMAT calculation for admitted:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "p(GMAT∣admit) &= \\frac{1}{\\sqrt{2 \\pi \\sigma^{2}_{GMAT|admit}}} exp\\Big({-\\frac{(GMAT - \\mu_{GMAT|admit})^{2}}{2\\sigma^{2}_{GMAT|admit}}}\\Big) \\\\ \\\\\n",
    "&= \\frac{1}{\\sqrt{2 \\pi (3143)}} exp\\Big({-\\frac {(384 - 518.50)^{2}}{2(3143)}}\\Big) \\\\ \\\\\n",
    "&=\\mathbf{.0004}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "GPA calculation for admitted:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "p(GPA∣admit) &= \\frac{1}{\\sqrt{2 \\pi \\sigma^{2}_{GPA|admit}}} exp\\Big({-\\frac{(GPA - \\mu_{GPA|admit})^{2}}{2\\sigma^{2}_{GPA|admit}}}\\Big) \\\\ \\\\\n",
    "&= \\frac{1}{\\sqrt{2 \\pi (0.0193)}} exp\\Big({-\\frac {(2.90 - 3.15)^{2}}{2(0.0193)}}\\Big) \\\\ \\\\\n",
    "&=\\mathbf{0.568767}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "GMAT calculation for not-admitted:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "p(GMAT∣!admit) &= \\frac{1}{\\sqrt{2 \\pi \\sigma^{2}_{GMAT|!admit}}} exp\\Big({-\\frac{(GMAT - \\mu_{GMAT|!admit})^{2}}{2\\sigma^{2}_{GMAT|!admit}}}\\Big) \\\\ \\\\\n",
    "&= \\frac{1}{\\sqrt{2 \\pi (1128.25)}} exp\\Big({-\\frac {(384 - 409.25)^{2}}{2(1128.25)}}\\Big) \\\\ \\\\\n",
    "&=\\mathbf{0.00895364}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "GPA calculation for not-admitted:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "p(GPA∣!admit) &= \\frac{1}{\\sqrt{2 \\pi \\sigma^{2}_{GPA|!admit}}} exp\\Big({-\\frac{(GPA - \\mu_{GPA|!admit})^{2}}{2\\sigma^{2}_{GPA|!admit}}}\\Big) \\\\ \\\\\n",
    "&= \\frac{1}{\\sqrt{2 \\pi (0.0474)}} exp\\Big({-\\frac {(2.90 - 2.610)^{2}}{2(0.0474)}}\\Big) \\\\ \\\\\n",
    "&=\\mathbf{0.7546488}\n",
    "\\end{align*}\n",
    "$$\n",
    "<br>   \n",
    "\n",
    "\n",
    "\n",
    "Then, plugging in values into the posterior expression, class probabilities for **ID=000000009** are given by:   \n",
    "<br>   \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "P(admit|data) &= \\frac{P(admit)P(GPA|admit)P(GMAT|admit)}{P(data)} \\\\ \\\\\n",
    "&= \\frac {(.5)*(0.568767)*(.0004)}{(0.568767)*(.0004)*(.5) + (0.7546488)* (0.00895364)*(.5)} \\\\ \\\\\n",
    "&= \\mathbf{0.03266} \\\\ \\\\\n",
    "P(!admit|data) &= \\frac{P(!admit)P(GPA|!admit)P(GMAT|!admit)}{P(data)} \\\\ \\\\\n",
    "&= \\frac {(.5)*(0.7546488)*(0.00895364)}{(0.568767)*(.0004)*(.5) + (0.7546488)* (0.00895364)*(.5)} \\\\ \\\\\n",
    "&= \\mathbf{0.967340} \\\\ \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "<br> \n",
    "<br> \n",
    "Thus, an individual with $GPA=2.90$ and $GMAT=384$ would almost certainly not be admitted according to the Gaussian Naive Bayes classifier.\n",
    "<br>   \n",
    "<br>  \n",
    "\n",
    "\n",
    "\n",
    "## k-Nearest Neighbors\n",
    "\n",
    "The principle behind nearest neighbor methods is to find a predefined number of training samples closest in distance to the new point, and predict the label from these. The number of samples can be a user-defined constant (k-nearest neighbor learning), or vary based on the local density of points (radius-based neighbor learning). The distance can, in general, be any metric measure: standard Euclidean distance is the most common choice.  \n",
    "<br>    \n",
    "Classification is computed from a simple majority vote of the nearest neighbors of each point: a query point is assigned the data class which has the most representatives within the nearest neighbors of the point.\n",
    "<br>\n",
    "\n",
    "\n",
    "To determine the optimal value for `n_neighbors`, the kNN hyperparameter, use the \n",
    "`model_selection.GridSearchCV` class:  \n",
    "<br>\n",
    "```python\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "param_grid = [{\n",
    "        'n_neighbors': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20]\n",
    "        }]\n",
    "\n",
    "knn_clf = KNeighborsClassifier()\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "                    knn_clf, param_grid, cv=5, scoring='neg_mean_squared_error'\n",
    "                    )\n",
    "                    \n",
    "# fit estimator to training data with optimal `n_neighbors` hyperparameter =>\n",
    "grid_search.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    " \n",
    "#### [k-Nearest Neighbors]\n",
    "\n",
    "<br>\n",
    "<a href=\"http://www.youtube.com/watch?feature=player_embedded&v=ZWygMcenuWM\n",
    "\" target=\"_blank\"><img src=\"http://img.youtube.com/vi/ZWygMcenuWM/0.jpg\" \n",
    "alt=\"kNN\" width=\"350\" height=\"275\" border=\"0\" /></a>\n",
    "<br>\n",
    "\n",
    "\n",
    "## Logistic Regression\n",
    "\n",
    "\n",
    "The `linear_model.LogisticRegression` class implements regularized Logistic Regression. Without offsetting the regularization and intercept scaling parameters, coefficent estimates generated by sklearn's `linear_model.LogisticRegression` *will not* align with estimates generated by `glm` in R. \n",
    "<br>\n",
    "\n",
    "The `LogisticRegression` classifier cannot handle categorical explanatory variables with more than 2 classes. If a categorical explanatory variable has more than 2 classes, it must be transformed using the '1-vrs-all' or 'one-hot' encoding schemes.     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ================================================================================\n",
    "# Logistic Regression parameter estimation demonstration, with and without       |\n",
    "# Regularization and intercept scaling.                                          | \n",
    "# ================================================================================\n",
    "import numpy as np\n",
    "\n",
    "# prepare Challenger data =>\n",
    "dfc = pd.read_table(\n",
    "        \"S:\\\\public\\\\Actuarial\\\\DSSG\\\\20170721_Materials\\\\Challenger.csv\",\n",
    "        sep=\",\")\n",
    "\n",
    "# bind `TEMPERATURE` and response (`O_RING_FAILURE`) =>\n",
    "X = dfc.iloc[:,[1]].as_matrix()\n",
    "y = dfc['O_RING_FAILURE'].values\n",
    "\n",
    "# ================================================================================\n",
    "# Model I: No dampening of regularization parameter (`C`) or intercept scaling.  |\n",
    "#          The coefficients estimated by Model I will not match those estimated  | \n",
    "#          by R's glm function.                                                  |            \n",
    "# ================================================================================\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr_init = LogisticRegression().fit(X, y)\n",
    "\n",
    "# get coefficients estimated from training data =>\n",
    "coeffs_init = (lr_init.intercept_[0], lr_init.coef_[0][0])\n",
    "\n",
    "coeffs_init\n",
    "\n",
    "# (0.52055517729376832, -0.021002151714167645)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ================================================================================\n",
    "# Model II: Dampen regularization by setting C to a large positive value: `C`    |\n",
    "#           represents the  inverse of regularization strength; smaller values   |\n",
    "#           of C specify stronger regularization. Model II coefficient           |\n",
    "#           estimates should match with R's glm output when family = `binomial`  |\n",
    "#           and link = `logit`.                                                  |\n",
    "# ================================================================================\n",
    "lr_mod = LogisticRegression(C=1e10, intercept_scaling=200).fit(X, y)\n",
    "\n",
    "# get coefficients estimated from training data =>\n",
    "coeffs_mod = (lr_mod.intercept_[0], lr_mod.coef_[0][0])\n",
    "\n",
    "coeffs_mod\n",
    "\n",
    "# sklearn coeffs (no regularization): (15.042890754064651, -0.2321625838604314)\n",
    "# R glm coeffs   (binomial, logit)  : (15.0429016  ,       -0.2321627 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines\n",
    "\n",
    "A Support Vector Machine model is a representation of the examples as points in space, mapped so that the examples of the separate categories are divided by a clear gap that is as wide as possible. New examples are then mapped into that same space and predicted to belong to a category based on on which side of the gap they fall.\n",
    "In addition to performing linear classification, SVMs can efficiently perform a non-linear classification using what is called the kernel trick, implicitly mapping their inputs into high-dimensional feature spaces.\n",
    "\n",
    "The `svm` class in sklearn is a wrapper around the C libraries `LIBSVM` and `LIBLINEAR`, popular utilities for support vector machines and large linear classification.\n",
    "\n",
    "<br>\n",
    "\n",
    "In sklearn, the linear support vector classifier is implemented separately from the \n",
    "general purpose SVM classifier:\n",
    "\n",
    "```python\n",
    "# sample usage of LinearSVC classifier =>\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "linear_svm_clf = LinearSVC(C=1.0).fit(X, y)\n",
    "\n",
    "# get support vectors =>\n",
    "linear_svm_clf.support_vectors_\n",
    "\n",
    "# array([[ 0.,  0.],\n",
    "#        [ 1.,  1.]])\n",
    "\n",
    "# get number of support vectors for each class\n",
    "clf.n_support_ \n",
    "\n",
    "# array([1, 1]...)\n",
    "```\n",
    "\n",
    "For datasets that are not linearly seperable, or when dealing with a non-linear decision function, use the general `svm.SVC` class.\n",
    "\n",
    "Note that `svm.SVC(kernel='linear')` is essentially the same classifier as `svm.LinearSVC` above, but `svm.LinearSVC` has superior performance characteristics, since it's optimized to solve a single type of optimization problem. \n",
    "\n",
    "The most common kernel used with non-linear SVM's is the 'Radial Basis Function' or 'rbf' kernel. \n",
    "\n",
    "Support Vector Machines are powerful tools, but their compute and storage requirements increase rapidly with the number of training vectors.\n",
    "\n",
    "Here's a comparison of estimator throughput per unit time:   \n",
    "<br>\n",
    "\n",
    "<p align=\"center\">\n",
    "  ![Throughput](latency.png)\n",
    "</p>\n",
    "\n",
    "\n",
    "## Implementing Classifiers \n",
    "\n",
    "The sample dataset (`mw.data`) contains ~150 records, with each records classified as either `male` (1) or `female` (2). The fields in the dataset are:\n",
    "\n",
    "* gender (response)\n",
    "* height (mm)\n",
    "* Hand length (mm)  \n",
    "* forearm length (mm) \n",
    "\n",
    "Prior to fitting the classifiers, we need to preprocess our features. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# Setup and Preprocessing           |\n",
    "# ===================================\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "sns.set(style='whitegrid', context='notebook')\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 9)\n",
    "\n",
    "\n",
    "# read in dataset =>\n",
    "fpath = \"S:\\\\public\\\\Actuarial\\\\DSSG\\\\20170721_Materials\\\\mw.data\"\n",
    "hdrs  = [\"ID\", \"GENDER\", \"HEIGHT\", \"HAND_LENGTH\", \"FOREARM_LENGTH\"]\n",
    "df    = pd.read_table(fpath, sep=\"\\s+\", names=hdrs)\n",
    "\n",
    "\n",
    "# split explanatory variables from response, and convert\n",
    "# 1/2 response to 0/1 =>\n",
    "X = df.drop(['GENDER','ID'], axis=1)\n",
    "y = df['GENDER'].map(lambda x: 0 if x==2 else x).values\n",
    "\n",
    "\n",
    "# split data into training and test sets...\n",
    "# NOTE: replace `cross_validation` with `model_selection` in \n",
    "#       the latest release of scikit-learn =>\n",
    "from sklearn.cross_validation  import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "                                X, y, test_size=.33, random_state=16)\n",
    "\n",
    "\n",
    "# scale explanatory variables =>\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sclr = StandardScaler()\n",
    "X_train = sclr.fit_transform(X_train)\n",
    "X_test  = sclr.transform(X_test)\n",
    "\n",
    "\n",
    "# take a look at preprocessed data =>\n",
    "X_train[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#==============================================\n",
    "# Gaussian Naive Bayes Classifier             |\n",
    "#==============================================\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "nb_clf = GaussianNB().fit(X_train, y_train)\n",
    "\n",
    "# get estimated class predictions and probabilities =>\n",
    "nb_y_hat = nb_clf.predict(X_test)\n",
    "nb_p_hat = nb_clf.predict_proba(X_test)[:,1]\n",
    "\n",
    "# generate classification report =>\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, nb_y_hat, target_names=['Male', 'Female']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#=============================================\n",
    "# kNN Classifier                             |\n",
    "#=============================================\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn_clf = KNeighborsClassifier(n_neighbors=5).fit(X_train, y_train)\n",
    "\n",
    "# get estimated class predictions and probabilities =>\n",
    "knn_y_hat = knn_clf.predict(X_test)\n",
    "knn_p_hat = knn_clf.predict_proba(X_test)[:,1]\n",
    "\n",
    "# generate classification report =>\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, knn_y_hat, target_names=['Male', 'Female']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#=============================================\n",
    "# Logistic Regression Classifier             |\n",
    "#=============================================\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr_clf = LogisticRegression(C=1).fit(X_train, y_train)\n",
    "\n",
    "# pass test set to classifier to evaluate model fit =>\n",
    "lr_y_hat = lr_clf.predict(X_test)\n",
    "lr_p_hat = lr_clf.predict_proba(X_test)[:,1]\n",
    "\n",
    "# generate classification report =>\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, lr_y_hat, target_names=['Male', 'Female']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#=============================================\n",
    "# Linear SVM Classifier                      |\n",
    "#=============================================\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_clf = SVC(C=1.0, probability=True).fit(X_train, y_train)\n",
    "\n",
    "# pass test set to classifier to evaluate model fit =>\n",
    "svm_y_hat = svm_clf.predict(X_test)\n",
    "svm_p_hat = svm_clf.predict_proba(X_test)[:,1]\n",
    "\n",
    "# generate classification report =>\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, svm_y_hat, target_names=['Male', 'Female']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#=============================================\n",
    "# Random Forest Classifier                   |\n",
    "#=============================================\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_clf = RandomForestClassifier().fit(X_train, y_train)\n",
    "\n",
    "# pass test set to classifier to evaluate model fit =>\n",
    "rf_y_hat = rf_clf.predict(X_test)\n",
    "rf_p_hat = rf_clf.predict_proba(X_test)[:,1]\n",
    "\n",
    "# generate classification report =>\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, rf_y_hat, target_names=['Male', 'Female']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#=============================================\n",
    "# AdaBoost Classifier                        |\n",
    "#=============================================\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "ab_clf = RandomForestClassifier().fit(X_train, y_train)\n",
    "\n",
    "# pass test set to classifier to evaluate model fit =>\n",
    "ab_y_hat = ab_clf.predict(X_test)\n",
    "ab_p_hat = ab_clf.predict_proba(X_test)[:,1]\n",
    "\n",
    "# generate classification report =>\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, ab_y_hat, target_names=['Male', 'Female']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voting Classifier\n",
    "\n",
    "A very simple way to create a superior classifier is to aggregate the predictions of\n",
    "many classifiers and predict the class that gets the most votes. This majority-vote classifier is called a 'hard voting classifier'.\n",
    "\n",
    "If all classifiers are able to estimate class probabilities (i.e., they have a \n",
    "`predict_proba()` method), then you can tell the voting classifier to predict the class with the highest class probability, averaged over all the individual classifiers. This is called 'soft voting'. It often achieves higher performance than hard voting because it gives more weight to highly confident votes. All you need to do is replace `voting='hard'` with `voting='soft'` and ensure that all classifiers can estimate class probabilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#=============================================\n",
    "# Voting Classifier (Ensemble Learner)       |\n",
    "#=============================================\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# instantiate models with known `predict_proba` attribute =>\n",
    "rf_clf  = RandomForestClassifier().fit(X_train, y_train)\n",
    "lr_clf  = LogisticRegression().fit(X_train, y_train)\n",
    "nb_clf  = GaussianNB().fit(X_train, y_train)\n",
    "knn_clf = KNeighborsClassifier().fit(X_train, y_train)\n",
    "\n",
    "# initialize voting classifier, providing an abbr. for each model =>\n",
    "voting_clf = VotingClassifier(\n",
    "            estimators=[('nb', nb_clf),('lr', lr_clf),('rf', rf_clf),('knn', knn_clf)],\n",
    "            voting='soft'\n",
    "            )\n",
    "\n",
    "# If voting=‘soft’, predicts the class label based on the argmax \n",
    "# of the sums of the predicted probabilities, which is \n",
    "# recommended for an ensemble of well-calibrated classifiers.\n",
    "\n",
    "# train voting classifier =>\n",
    "voting_clf.fit(X_train, y_train)\n",
    "\n",
    "# print accuracy score for all classifiers =>\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "for clf in (lr_clf, rf_clf, knn_clf, nb_clf, voting_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "## Classifier Assessment\n",
    "\n",
    "**Precision** is a measure of the accuracy of positive predictions.   \n",
    "**Recall** is a measure of the number of positive instances detected by the classifier.   \n",
    "\n",
    "* *True/False* refers to whether the model's predicition is correct or incorrect.\n",
    "* *Positive/Negative* refers to whether the model predicted the positive or negative class. \n",
    "* *P* is the number of positive instances in the actual dataset.\n",
    "* *N* is the number of negative instances in the actual dataset.\n",
    "\n",
    "It immediately follows that:\n",
    "\n",
    "* **True Positive** (TP) - The model of interest correctly predicts the positive class.\n",
    "* **True Negative** (TN) - The model of interest correctly predicts the negative class.\n",
    "* **False Positive** (FP) - The model of interest incorrectly predicts the positive class (Type I error).\n",
    "* **False Negative** (FN) - The model of interest incorrectly predicts the negative class (Type II error).\n",
    "\n",
    "*Accuracy* is defined as:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "ACC = \\frac {TP+TN}{P+N} = \\frac {TP+TN}{TP+TN+FP+FN}\n",
    "\\end{aligned}\n",
    "$$\n",
    "    \n",
    "<br>\n",
    "\n",
    "*Precision* is the fraction of positive predictions that are correct:\n",
    "\n",
    "$$       \n",
    "\\begin{aligned}\n",
    "Precision = \\frac{TP}{TP+FP}\n",
    "\\end{aligned}\n",
    "$$\n",
    " \n",
    "<br>\n",
    "\n",
    "   \n",
    "*Recall* (True Positive Rate) is the fraction of all positive instances the classifier correctly predicts as positive: \n",
    "\n",
    "$$  \n",
    "\\begin{aligned}\n",
    "Recall = TPR = \\frac{TP}{TP+FN}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "*False Positive Rate* (Type-I error) is the fraction of all negative instances the classifier incorrectly identifies as positive:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "FPR = \\frac{FP}{TN+FP}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "Precision and recall are used in the $F_{1}$ score, which is defined as the harmonic mean of precision and recall:\n",
    "\n",
    "$$   \n",
    "\\begin{aligned}\n",
    "F_{1} = 2 \\frac{Precision * Recall}{Precision + Recall}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "### Confusion Matrix\n",
    "The *confusion matrix* can be used to evaluate the accuracy of a classification. From the scikit-learn [docs](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html):\n",
    "\n",
    "*By definition a confusion matrix $C$ is such that $C_{i, j}$ is equal to the number of observations known to be in group i but predicted to be in group j.\n",
    "\n",
    "$$\n",
    "C_{0, 0} = True  Negatives \\\\\n",
    "C_{1, 0} = False Negatives \\\\\n",
    "C_{1, 1} = True  Positives \\\\\n",
    "C_{0, 1} = False Positives \\\\\n",
    "$$\n",
    "\n",
    "\n",
    "Thus in binary classification, the count of true negatives is $C_{0,0}$, false negatives is $C_{1,0}$, true positives is $C_{1,1}$ and false positives is $C_{0,1}$.*\n",
    "\n",
    "  \n",
    "### ROC Curve\n",
    "The [ROC curve](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) (Receiver Operating Characteristic) is a plot often used used in assessing the quality of a binary classifier as the discrimination threshold is varied. The ROC curve uses $TPR$ and $FPR$. Convention dictates plotting $TPR$ as a function of $FPR$, with $TPR$ on the y-axis and $FPR$ along the x-axis. \n",
    "\n",
    "The area under the ROC curve (AUC) is used to test model quality. The higher the AUC, the better the model. A high area under the curve represents both high recall and high precision, where high precision relates to a low false positive rate, and high recall relates to a low false negative rate. High scores for both show that the classifier is returning accurate results (high precision), as well as returning a majority of all positive results (high recall).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ====================================\n",
    "# Display Confusion Matrix           |\n",
    "# ====================================\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "actual_response    = y_test\n",
    "predicted_response = knn_y_hat\n",
    "\n",
    "cm = confusion_matrix(actual_response, predicted_response)\n",
    "\n",
    "sns.heatmap(cm, square=True, annot=True, cbar=False)\n",
    "plt.xlabel('Predicted Value')\n",
    "plt.ylabel('Actual Value')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "#  Plot ROC curve for each classifier      |\n",
    "# ==========================================\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "fpr1, tpr1, thresholds1 = roc_curve(y_test, rf_p_hat, pos_label=1)\n",
    "fpr2, tpr2, thresholds2 = roc_curve(y_test, lr_p_hat, pos_label=1)\n",
    "fpr3, tpr3, thresholds3 = roc_curve(y_test, nb_p_hat, pos_label=1)\n",
    "fpr4, tpr4, thresholds4 = roc_curve(y_test, knn_p_hat, pos_label=1)\n",
    "fpr5, tpr5, thresholds5 = roc_curve(y_test, ab_p_hat, pos_label=1)\n",
    "\n",
    "plt.plot(fpr1, tpr1, linewidth=2, label='Random Forest')\n",
    "plt.plot(fpr2, tpr2, linewidth=2, label='Logistic')\n",
    "plt.plot(fpr3, tpr3, linewidth=2, label='Naive Bayes')\n",
    "plt.plot(fpr4, tpr4, linewidth=2, label='kNN')\n",
    "plt.plot(fpr5, tpr5, linewidth=2, label='Ada Boost')\n",
    "plt.plot([0,1], [0,1], '--')\n",
    "plt.axis([-.05, 1.05, -.05, 1.05])\n",
    "plt.xlabel('False Positive Rate (FPR)')\n",
    "plt.ylabel('True Positive Rate (TPR)')\n",
    "plt.grid(True)\n",
    "plt.legend(loc='lower right',prop={'size':20}, frameon=True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# print AUC Score for kNN classifier          |\n",
    "# =============================================\n",
    "print(\"kNN classifier AUC Score: {}\".format(roc_auc_score(y_test, knn_p_hat)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
